
# 데이터 탐색  및 인코딩
## 기본 라이브러리 임포트

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

## 데이터 불러오기

```python
df = pd.read_csv('')  # csv 파일
df = pd.read_excel('')  # excel 파일
df = pd.read_json('')  # json 파일
df = pd.read_xml('')  # xml 파일
```

## 데이터 탐색하기

```python
df.head()  # 데이터프레임 행 앞에서 5개까지 보기
df.tail()  # 데이터프레임 행 뒤에서 5개까지 보기
df.info()  # 데이터프레임 행/열 정보 보기
df.describe()  # 계산 가능한 값 통계 정보 보기
df.index   # index 로 보여지기
df.colums  # 데이터프레임 열 확인
df.values  # numpy가 인식가능하게 하기
```

## 결측치 확인하기

```python
df.isna().sum()  #  결측치 확인
df.isnull().sum()  # Null인 데이터 확인
```

## 특정 열의 값들의 건수/비율 확인하기

```python
df['A'].value_counts()    # 범주형 변수 각 범주별 빈도수 확인
df['B'].value_counts(normalize=True)   # 정규화된 값으로 확인
```

## 특정 열 삭제

```python
cols = ['A', 'B', 'C']   # 삭제할 컬럼을 변수에 담기
df = df.drop(cols, axis=1) # .drop()에 변수 담고 열(1) 삭제
# 또는
df.drop(cols, axis=1, inplace=True)  # 변수를 할당하지 않고 바로 적용
```

## 결측치 데이터 삭제

```python
df = df.dropna(subset=['sex_type'])  # .dropna(axis=0)
```

## 결측치 대체

```python
df.replace('변경전', '변경후', inplace=True)  # 하나만 대체
# 리스트 딕셔너리도 가능
df.replace({'변경전':'변경후', '변경전2':'변경후2'}, inplace=True)  # 여러개 대체

# 예제 (특정 칼럼 하나만 대체)
df['TotalCharges'] = df['TotalCharges'].replace([' '], ['0'], inplace=True)
```

## 결측치 처리

```python
- 결측치 많은 DeviceProtection 컬럼 제거
- Dependents 컬럼의 결측치 : 최빈값 채우기(mode)
- MonthlyCharges 컬럼의 결측치 : 평균값 채우기(mean)
- 나머지 결측치 제거(dropna)
# 1. 결측치 많은 DeviceProtection 컬럼 제거  --> drop 함수 활용
# 2. Dependents 컬럼의 결측치 : 최빈값 채우기  --> fillna 함수, mode 함수 활용
# 3. MonthlyCharges 컬럼의 결측치 : 평균값 채우기  --> fillna 함수, mean 함수 활용
# 4. 나머지 결측치 제거 : dropna 함수 활용

df.drop('DeviceProtection', axis=1)
df['Dependets'] = df['Dependents'].fillna( df['Dependents'].mode() )
df['MonthlyCharges'] = df['MonthlyCharges'].fillna( df['MonthlyCharges'].mean() )
df.dropna()
```

## 결측치 채우기

```python
df.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)

df.fillna('A')  # 결측 값 대체
df.fillna(value={'col1':'A', 'col2':'B'})   # 여러 값 대체
# fillna(method) : 다양한 방법으로 채우기 
# 'backfill'/'bfill': 바로 뒤에 값으로 채우기, 'pad'/'ffill': 바로 앞의 값으로 채우기
df.fillna(method='bfill')
f.fillna(method='ffill')

```

```python
median()
max()
min()
std()
```

## 데이터 타입에 따른 컬럼명

```python
df.select_dtypes(include='object').columns  # 원하는 데이터 타입에 해당하는 열 df 형태 확인
-> cols = df.select_dtypes('O').columns
```

## 그룹화

```python
# [문제] df 데이터프레임에서 숫자형 컬럼을 뽑아 'Churn' 컬럼을 기준으로 그룹화하고 각 컬럼의 평균을 출력 하세요.
df.select_dtypes(['int', 'float']).groupby(['Churn']).mean()
```

```python
# [문제] 아래 가이드에 따라 그룹화 하세요.
- 대상 컬럼 : 'gender' , 'tenure', 'MonthlyCharges', 'TotalCharges', 'Churn'
- Churn + gender 그룹화하여 각 평균 출력

# 'gender' , 'tenure', 'MonthlyCharges', 'TotalCharges', 'Churn' 컬럼을 대상으로
# Churn + gender 그룹화 및 평균 출력
# 이탈하지 않은 남녀는 비슷하게 총요금을 사용하고 있으나,
# 이탈하는 고객중에서 남자가 여자보다 총요금을 더 사용하고 있습니다.

df[ ['gender' , 'tenure', 'MonthlyCharges', 'TotalCharges', 'Churn'] ].groupby([ 'Churn' , 'gender' ]).mean()

```

## 원핫 인코딩 / 라벨 인코딩

```python
from sklearn.preprocessing import LabelEncoder  # 라벨인코딩

le = LabelEncoder()
df['C'] = le.fit_transform(df['C'])

from sklearn.preprocessing import OneHotEncoder  # 원핫인코딩

oe = OneHotEncoder()
df['D'] = oe.fit_transform(df['D'])
```

## 원-핫 인코딩 (get_dummies)

```python
방법 1)
cols = ['', '']  # 인코딩할 변수 칼럼에 담기
df = pd.get_dummies(df, columns=cols)
# 또는
방법 2)
df = pd.get_dummies(df, columns=['one', 'two'], drop_first=True)
# drop_first=True 는 첫번째 범주 제외 원핫인코딩

# 선택한 다중 칼럼 원핫 인코딩 문제
문제1. 원-핫인코딩은 범주형 변수를 1과 0의 이진형벡터로 변환하기 위해 사용하는 원-핫인코딩으로 아래 조건에 해당하는 컬럼데이터를 변환하세요.  
• 대상데이터프레임: media_label  
• 원-핫 인코딩 대상컬럼: ‘sex_cd’ : 성별코드, ‘age_cd: 연령코드  
• 활용함수: Pandas의 get_dummies  
• drop first옵션을 통해 가변수(dummy variable) 1개 삭제  
• 해당 전처리가 반영된 media_label를 데이터프레임변수 media_preset에 저장해 주세요 

#어기에 답안코드를 작성하세요  
media_preset = pd.get_dummies(data=media_label, columns=[‘sex_cd’, ‘age_cd’], drop_first=True)

방법 3) 선택한 두개의 컬럼만을 제외하고 원핫인코딩을 진행 후 다시 합쳐야 할 때 (concat 사용)

media_label = pd.DataFrame(data)
# 원-핫 인코딩을 제외할 컬럼을 따로 저장
ex_cols = ['sex_cd', 'age_cd']
# 제외할 컬럼을 드롭하고 원-핫 인코딩을 적용
media_preset = pd.get_dummies(media_label.drop(columns=ex_cols), drop_first=True)
# 제외한 컬럼을 다시 합침
media_preset = pd.concat([media_label[ex_cols], media_preset], axis=1)
# 결과 출력
print(media_preset)
```


# 데이터 시각화 그래프

## 히스토그램

### matplotlib

```python
#age에 대한 빈도를 20개 구간으로 그리기
plt.figure()
plt.hist(df["age"],bins=20)
plt.show()

#service수 별 A,B 서비스 요금에 대한 막대 그래프 그리기
df2[['A_bill', 'B_bill']].plot(kind='bar')

#service수 별 A,B 서비스 요금에 대한 히스토그램 그래프 그리기
df2[['A_bill', 'B_bill']].plot(kind='hist')

# 막대 그래프
df['A'].value_counts().plot(kind='bar') 
```

## seaborn(sns)

```python
sns.histplot(data=df, x='', hue='') # histogram
sns.histplot(x=df['A'], y=df['B']) # histo 컬럼 비교

sns.jointplot(data=df, x='') # 산점도 

sns.countplot(data=df, x='') # 발생 횟수

sns.kdeplot(x=df['A']) # 밀도 추정치(연속된 곡선)

heat = df[['A', 'B', 'C']].corr() # 히트 변수 만들기 
sns.heatmap(heat, annot=True, cmap='') # 히트맵

sns.boxplot(x=df['A'])  # 특이치 발견
sns.boxplot(data=df, x='', y='') # 컬럼 두개도 가능
```

# 데이터 분리하기

```python
data = pd.read_csv('$$.csv')
x_data = data.drop('##', axis=1)
y_data = data.loc[:, '##']
```

## train_test_split

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = trian_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)   # stratify는 분류에서 사용
```

## 데이터 표준화/정규화

```python
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train = mms.fit_transform(X_train)
X_test = mms.transform(X_test)

# 만약 feature_imortance 를 사용한다면
fi = mms.feature_importances_
pd.Series(fi, index=X.columns).sort_values(ascending=False).plot(kind='bar')



```


# 머신러닝 모델링

## 분류
### 1) 분류 라이브러리 & 모델 선언

```python
# 로지스틱 회귀
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter= , C= )

# K-이웃 분류
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors= )

# 의사결정나무 분류
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=, random_state= )

# 랜덤포레스트 분류
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=, random_state= )

# XGB Boost 분류
from xgboost import XGBClassifier
model = XGBClassifier(n_estimators=)

# LGB Boost 분류
from lightgbm import LGBClassifier
model = LGBClassifier(n_estimators=)

# * n_estimators 는 사용하는 결정 트리 개수 이다.
```

### 2) feature importance
**변수중요도(Feature_importances_)**

```python
# 랜덤포레스트 모델의 38개의 Feature에 대한 중요도(feature_importances_) 보기
rf.feature_importances_
```

```python
# 랜덤포레스트 모델의 변수중요도를 변수별로 중요도를 상위 7개 출력
# Series 함수 이용 : adata=rf.feature_importances_ , index=X.columns
# sort_values 함수 활용하여 오름차순으로 정렬
# 상위 7개만 보기

pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(7)
```

```python
# 랜덤포레스트 모델의 변수중요도를 변수별로 중요도를 bar 차트 출력
# Series 함수 이용 : data=rf.feature_importances_ , index=X.columns
# sort_values 함수 활용하여 오름차순으로 정렬
# bar 차트 그리기

적용한다면??
# 변수 중요도 계산
importances = rf.feature_importances_

# 변수 중요도를 시리즈로 변환 1)
importance_series = pd.Series(importances, index=X.columns)

# 변수 중요도를 오름차순으로 정렬하고 바 차트로 시각화 2)
importance_series.sort_values(ascending=False).plot(kind='bar')

# 1) , 2) 합침
pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).plot(kind='bar')

```

### 3) 분류 모델 학습

```python
model.fit(X_train, y_train, epochs=, batch_size, validation_data=(,))
```

### 4) 분류 성능평가

```python
# accuracy, precision, recall, f1
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred = model.predict(X_test)  # 예측 모델 대상: X_test

# 결과
print(accuracy_score(y_test, y_pred))
print(recall_score(y_test, y_pred))
print(precision_score(y_test, y_pred))
```

### 5) 분류 히트맵

```python
방법1) 기본
# 분류하고 성능평가 했다면 히트맵으로 시각화 진행
heat = confusion_matrix(y_test, y_pred)

sns.heatmap(heat, annot=True, fmt='d', color='Blues')

방법2) 특정 칼럼 상관관계
# 판다스 corr 함수 이용해서 'tenure','MonthlyCharges','TotalCharges' 컬럼간의 상관관계를 확인해 보자
# 상관관계 결과를 corr 변수에 저장하고 내용 확인하자.
df[['tenure','MonthlyCharges','TotalCharges']].corr()

# tenure','MonthlyCharges','TotalCharges' 컬럼간의 상관관계를 seaborn heatmap으로 그려보자.(annot 추가)
# 분석결과 : tenure(서비스 사용기간)과 TotalCharges(서비스 총요금)간의 깊은 상관관계가 있어 보인다.
sns.heatmap(df[['tenure','MonthlyCharges','TotalCharges']].corr(), annot=True)
```


## 회귀

### 1) 회귀 라이브러리 & 모델 선언

```python
# 로지스틱 회귀
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter= , C= )

# K-이웃 분류
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsResgressor(n_neighbors= )

# 의사결정나무 분류
from sklearn.tree import DecisionTreeResgressor
model = DecisionTreeResgressor( )

# 랜덤포레스트 분류
from sklearn.ensemble import RandomForestResgressor
model = RandomForestResgressor( )

# XGB Boost 분류
from xgboost import XGBResgressor
model = XGBResgressor( )

# LGB Boost 분류
from lightgbm import LGBResgressor
model = LGBClassifier( )
```

### 2) 회귀 학습

```python
model.fit(X_train, y_train, epochs=, batch_size, validation_data=(,), callbacks=[,])
```

### 3) 회귀 성능평가

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# 결과
print(mean_squared_error(y_test, y_pred, squared=False)) # RMSE를 반환
print(mean_absolute_error(y_test, y_pred))
print(r2_score(y_test, y_pred))

# 회귀 계수 = 가중치 확인 :
model.coef_

# 편향(y절편) = 
model.intercept_
```


# 딥러닝 모델링

### 라이브 불러오기

```python
import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Dense, Dropout,Input,Activation, BatchNormalization,Flatten
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.backend import clear_session
clear_session()
```

### 모델 입력값 출력값 확인

```python
X_train.shape  # 입력값 확인
y_train.shape  # 출력값 확인
```

### DNN 모델링

```python
model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1])))  # relu: 다중 분류
model.add(Dropout(0.2))  # 드롭아웃 0.2
model.add(Dense(8, activation='sigmoid',))  # sigmoid: 이진 분류
model.add(BatchNormalization())  # BatchNormalization 추가
model.add(Dense(1, activation='softmax'))  # softmax

# earlustopping 조기 종료 (과대적합 방지)
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

# modelcheckpoint 가중치 저장 (최적 모델 선정)
ms = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True, )

# model 컴파일 - loss, optimizer, metrics 순
model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])

# history 담기 <- model fit (학습) - train, epochs, batch_size, validation_data, callbacks, verbose 순
history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test))

# model 요약
model.summary()
```

### 성능 평가 (분류)

```python
# 이진분류
y_pred = model.predict(x_test)
y_pred_x = np.where(y_pred>=0.5, 1,0 )
print(classification_report(y_test, y_pred_x))

# 다중분류
y_pred = model.predict(x_test)
y_pred_x= np.argmax(y_pred,axis=1)
print(classification_report(y_test, y_pred_x))
```

### 학습 / 검정 손실 그래프

```python
plt.plot(history.history['mse'])
plt.plot(history.history['val_mae'])
plt.title('Model MSE')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.show()
```




